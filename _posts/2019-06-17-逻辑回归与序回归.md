---
layout: post
title: 逻辑回归与序回归
category: [机器学习]
description: 简单介绍序回归的基本概念，稍微深入分析逻辑回归特别是多项逻辑回归背后的合理性，以及序回归和逻辑回归的联系和差异
---
## 二项逻辑回归与多项逻辑回归
我们说到逻辑回归多半是指的二项逻辑回归，也就是通常说的二分类问题，关于二分类问题的具体解释，我不再进行说明，我们用$$y=\{0,1\}$$来表示两种类别的被解释变量，使用$$X$$表示解释变量，逻辑回归的模型表示为：

$$
p(y=1\vert X) = \frac{e^{w^TX}}{1+e^{w^TX}}
$$

其中$$\frac{e^{w^TX}}{1+e^{w^TX}}$$被称为*Sigmoid*函数也被称为*Logistic*函数。关于*Sigmoid*函数，我们需要区分下列三个函数：
1. *Sigmoid*函数
2. *Logit*函数
3. *Probit*函数

这三个函数经常出现在逻辑回归的相关文献中，而很多文献并没有对此作出相应的解释，导致很多人（比如我）在阅读时，经常弄混淆三者，弄得云里雾里。*Sigmoid*函数刚才已经说明了，形式是$\frac{1}{1+e^{w^TX}}$，而与它概念接近的是*Logit*函数，*Logit*函数是*Sigmoid*函数的反函数，它表达的含义是将概率Probility函数转变成Odd函数，因为Odd函数的取值范围为$$[0,\infty]$$，从而进行实数轴上的映射，Odd函数的表达形式为：

$$
odd = \frac{p}{1-p}
$$

因此，逻辑回归的问题也可以转换成如下形式：

$$
\begin{aligned}
   \log(odd) &= w^TX\\
   \log(\frac{p}{1-p}) &= w^TX 
\end{aligned}
$$

经过简单的换算，我们可以看出对于二分类逻辑回归，*Sigmoid*函数和*Logit*函数是互为反函数的。

Probit函数，需要从另一个角度来理解*Sigmoid*函数，*Sigmoid*函数从概率分布的角度上看，我们可以发现，其本质上是一个概率分布函数模型，即

$$
p(y=1\vert X) = f(X)
$$

我们选择了$$f(x) = \frac{1}{1+e^{w^TX}}$$这个形式的分布（Logistic分布），因此我们可以任意假设$$f(x)$$的模型，如果$$f(x)$$是正态分布的概率分布函数，那么我们称该模型为*Probit*函数模型

$$
p(y=1\vert X) = \Phi(w^TX)
$$

对于*Sigmoid*和*Probit*函数来说，$$p(y=1\vert X)$$这个的表示方法均代表$$P(y^*\geq Threshold)$$ 其中 $$y^* =w^TX+\epsilon $$，也就是说对于逻辑回归问题，实际上是在拟合一个最佳的概率分布，而这个分布的形式是由事先决定好的，而拟合的目标是基于这个形式下的最优参数。

## 逻辑回归的估计方法
我们经常听到逻辑回归的拟合方式是极大似然估计，但是对于我这种对名词不敏感的人来说，还是数学公式好理解一些，而极大似然估计本身也可以说很大的篇章了，因为其原理并不是那么简单和易懂，并且涉及到一些贝叶斯统计的思想，在这篇文章中暂时不去深究了（另抽些时间来写写贝叶斯估计的东西）

我们用机器学习中常用的概念，损失函数来说明逻辑回归的估计方法，因为对于任意的参数估计方法，最终体现到数学语言上都是一个最优化问题，在机器学习领域中，优化函数也被称为损失函数。
对于损失函数我们先确定单个样本的损失函数表达式。我们还是先看两分类下的逻辑回归问题吧。

两分类下的逻辑回归问题，对于单个样本来说，假设其真实类别为$$k,k\in\{0,1\}$$，而拟合的概率数值为$$p\in[0,1]$$，假设模型完美分类，那么这个样本的拟合概率$$p=0$$如果真是类别为$$0$$，对于真实类别为$$1$$的样本同理。那么我们可以定义出损失函数为:

$$
cost_i = \begin{cases}
    -\log(p)&\text{若} y_i=1\\
    -\log(1-p)&\text{若} y_i=0
\end{cases}
$$

如果真实分类为$$1$$，预测的$$p$$是1，那么损失函数就是0,如果预测的$$p=0$$，那么损失将接近于正无穷。若真实分类为0也是类似。

这里需要提一句，$$cost_i$$的形式选择我感觉是参考了极大似然估计的形式，为了两者达成一致，而$$-log(x)$$形式的损失函数也被称为对数损失函数

将上面的分段函数写成一个函数形式为：

$$
cost_i = -y_i*\log(p_i)-(1-y_i)*\log(1-p_i)
$$

那么总体损失函数

$$
Cost = \sum_{i=1}^{n}{cost_i}=\sum_{i=1}^{n}{-y_i*\log(p_i)-(1-y_i)*\log(1-p_i)}
$$

因此通过求解$$\min(Cost)$$可以得到最优参数。

对于多分类问题，似乎损失函数没有这么简单了。首先，多分类逻辑回归的模型基于两分类上的进行了相应扩展

$$
 p(y=k\vert X) =\displaystyle \frac{e^{w_k^TX}}{\sum_{i=1}^{m}{e^{w_i^TX}}}
$$

因此参数总数有$$m\times n$$这么多个（$$m$$是分类个数，$$n$$是解释变量个数），这一点需要注意

我们可以仿照二分类问题来写下单个样本在多分类条件下的损失函数

$$
cost_i = -\log(\frac{e^{w_k^TX}}{\sum_{i=1}^{m}{e^{w_i^TX}}}) \text{若} y_i=k
$$

可以写成为

$$
\begin{aligned}
    &cost_i = \sum_{k=1}^{m}{I(y_i=k)*-\log(\frac{e^{w_k^TX}}{\sum_{i=1}^{m}{e^{w_i^TX}}})}\\
    &I(y_i=k) = \begin{cases}
        1 & y_i=k\\
        0 & y_i\neq k
    \end{cases}
\end{aligned}

$$

总体损失函数

$$
Cost = \sum_{i=1}^{s}{cost_i} =  \sum_{i=1}^{s}{\sum_{k=1}^{m}{I(y_i=k)*-\log(\frac{e^{w_k^TX}}{\sum_{i=1}^{m}{e^{w_i^TX}}})}}
$$

对于这个损失函数的最小值优化，目前还不是很清楚方法，暂时不用管。

## 序回归
我们在考虑分类的时候，有时候还要考虑类别本身是具有顺序的