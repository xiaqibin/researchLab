---
layout: post
title:  反向传播算法概述
category: 数学 
description: 反向传播算法是神经网络训练算法中很基础的一个算法，其原理不是很复杂，仅仅是用了多元函数偏导的计算规则。但是网上的介绍多是描述性的介绍，因此这篇博客着重过程的推导，一步一步的来介绍所谓的反向传播算法。
---
### 正文
首先，我们要明确的是机器学习中一个很重要的概念，即，目标函数。常见的机器学习问题归根到底，是在解决一个优化问题，有些是凸优化，有些是非凸优化，优化问题准备找个时间专门写一个博客来介绍一下内容，在这里就不多说了。我们回到目标函数中来，在神经网络中，目标函数被称作代价函数或者成本函数，即对于单个样本(x,y)，x代表特征,y代表目标，代价函数为

$$J(W,b,x,y) = \frac{1}{2}\|h_{W,b}(x)-y\|_2$$

其中:

$$h_{W,b}(x)$$代表给定输入的变量$x$，经过神经网络模型后的输出为$$h_{W,b}(x)$$，$$W$$，$$b$$是待求解的模型变量
因此，对于所有的样本，我们都可以得到基于这个样本的代价函数，对这些代价函数求和，得到整体代价函数

$$J(W,b) =\frac{1}{n}\sum_{i=1}^n(\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2)$$

，同时为了防止过拟合，通常会加上参数的规则化项。最后整体代价函数就变成：

$$J(W,b) =\frac{1}{n}\sum_{i=1}^n{\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2}+\frac{\lambda}{2}\sum{W^2}$$

为了更好的来解释我们的模型，我们首先要约定一些参数的表示方法，在这之前，对于读者来说，我默认是了解过神经网络模型的基本结构，因此不再详细介绍了。

- $$W_{i,j}^{(l)}$$表示第$$l$$层，从上到下第$$j$$个单元，与第$$l+1$$层的第$$i$$个单元的联结参数。

- $$b_i^{(l)}$$表示第$$l+1$$层，第$$i$$单元的偏置项。

- $$S_l$$表示第$l$层的单元数目

- $$N_l$$表示模型网络层数

- $$a_i^l$$表示第$$l$$层，第$$i$$个单元的激活值

- $$z_i^l$$表示第$$l$$层，第$$i$$个单元的参数与上一层单元的激活值的线性组合

- $$f(x)$$表示激活函数，有$$a_i^l = f(z_i^l)$$


因此，可以看出来，对于任意两层之间的参数，都是一个参数二维矩阵。

好，我们回到整体代价函数上来，在经过约定的符号表示以后，整体代价函数可以重新表示为

$$
\begin{equation}
J(W,b) =\frac{1}{n}\sum_{i=1}^n{\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2}+\frac{\lambda}{2}\sum_{l=1}^{N_l}{\sum_{j=1}^{S_l}{\sum_{i=1}^{S_{l+1}}{(W_{ij}^{(l)})^2}}}
\end{equation}
$$

后面的推导都是基于这些符号，因此务必熟悉各个符号的含义。

既然叫反向传播，那么肯定是从最后一层开始往前计算，最后一层什么数据都没有，怎么计算？这里就需要初始化参数以求得一个初始化的最终值，通常使用一个正态分布$$N(0,\varsigma^2),\varsigma=0.01$$，因此给定一个样本(x,y)，首先进行向前传导，计算出网络中每个节点的激活值并得到输出$$h_{W,b}(x)$$。对于第$$l$$层，我们计算残差值$$\delta_i^l$$(注：这个残差代表的是该节点对最终输出$$h_{W,b}(x)$$与实际值$$y$$的残差的影响值，即偏导)，至于为什么要这样做，我想在后面再说。
因此，整个流程如下：
- 利用前向传导计算所有$$L_2,L_3,L_4,\cdots L_N$$的激活值
- 对第$$N_l$$层（输出层）的每个输出单元计算

$$
\begin{aligned}
\delta_i^{N_l}&=\frac{\partial}{\partial z_i^{N_l}}{\frac{1}{2}\|y-h_{W,b}(x)\|_2}\\
&=\frac{\partial}{\partial z_i^{N_l}}{\frac{1}{2}\|y-f(z_i^{N_l})\|_2}\\
&=(y-a_i^{N_l})f'(z_i^{N_l})
\end{aligned}
$$

- 对$$l = N_l-1,N_l-2,\cdots,2$$的各个层，第$$l$$层的第$$i$$个节点的残差计算方法

$$
\begin{aligned}
\delta_i^{N_l-1}&=\frac{\partial}{\partial z_i^{N_l-1}}{\frac{1}{2}\|y-h_{W,b}(x)\|_2}\\
&=\frac{1}{2}\sum_{j=1}^{S_{N_l}}{\frac{\partial}{\partial z_i^{N_l-1}}(y_j-a_j^{N_l})^2}\text{注：这里代表的意思是，对于多个输出值，对所有输出值的偏导是每个偏导的和}\\
&=\frac{1}{2}\sum_{j=1}^{S_{N_l}}{\frac{\partial}{\partial z_i^{N_l-1}}(y_j-f(z_j^{N_l}))^2}\\
&=\sum_{j=1}^{S_{N_l}}{(y_j-f(z_j^{N_l}))\cdot \frac{\partial f(z_j^{N_l})}{\partial z_i^{N_l-1}}}\\
&=\sum_{j=1}^{S_{N_l}}{(y_j-f(z_j^{N_l}))\cdot f'(z_j^{N_l}) \cdot \frac{\partial z_j^{N_l}}{\partial z_i^{N_l-1}}}\\
&=\sum_{j=1}^{S_{N_l}}{(y_j-a_j^{N_l})\cdot f'(z_j^{N_l}) \cdot \frac{\partial z_j^{N_l}}{\partial z_i^{N_l-1}}}\\
&=\sum_{j=1}^{S_{N_l}}{\delta_j^{N_l} \cdot \frac{\partial z_j^{N_l}}{\partial z_i^{N_l-1}}}\\
&=\sum_{j=1}^{S_{N_l}}{\delta_j^{N_l} \cdot \frac{\partial}{\partial z_i^{N_l-1}}\sum_{k=1}^{N_l-1}{W_{jk}^{N_l-1}f(z_k^{N_l-1})}}\text{注：里面和式中的偏导，只有当k=i时才不等于0}\\
&=\sum_{j=1}^{S_{N_l}}{\delta_j^{N_l} \cdot W_{ji}^{N_l-1}\cdot f'(z_i^{N_l-1})}
\end{aligned}
$$

因此可以得到一个结论，前面一个单元层的残差导数，是后一个单元层的残差导数的加权和，并且形成递归的关系，将上述的$$N_l$$和$$N_l-1$$替换成$$l$$与$$l-1$$，就得到一个递推关系

$$
\delta_i^{l-1} = \sum_{j=1}^{S_{l}}{\delta_j^{l} \cdot W_{ji}^{l-1}\cdot f'(z_i^{l-1})}
$$

- 计算偏导数
我们得到了几个关系，首先定义$$\delta_i^l = \frac{\partial}{\partial}{J(W,b,x,y)}$$，为了使用梯度下降法，求解最优解，我们要知道各个层的参数对目标函数的偏导，通过反向传播，我们可以得到

$$
\begin{aligned}
   \frac{\partial}{\partial W_{ij}^l}J(W,b,x,y) &= \frac{\partial}{\partial z_{i}^{l+1}}J(W,b,x,y)\cdot \frac{\partial z_i^{l+1}}{\partial W_{ij}^l}\\
    &=\delta_i^{l+1}\cdot a_j^l
\end{aligned}
$$

$$
\begin{aligned}
   \frac{\partial}{\partial b_{i}^l}J(W,b,x,y) &= \frac{\partial}{\partial z_{i}^{l+1}}J(W,b,x,y)\cdot \frac{\partial z_i^{l+1}}{\partial b_{i}^l}\\
    &=\delta_i^{l+1}
\end{aligned}
$$

有了各个层的单元的权重偏导，可以运用梯度下降法进行优化目标函数了，在这里就不多说了。
### 参考文献
1. [UFLDL Tutorial](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)