---
layout: post
title: 强化学习中贝尔曼方程与策略梯度定理
category: [数学,机器学习]
description: 强化学习Reinforcement Learning在一开始介绍概念的时候无法避开的一个核心概念是贝尔曼方程(Bellman Equation)，它是整个强化学习算法的基础，读文献的时候这里有点不清不楚的，于是想找到一个比较好的方式来说明这个方程；梯度下降定理是一大类强化学习算法的一个基础，是强化学习进阶中无法避开的一个点，因此在这里把两者做一个详细的说明，以供后来参考。
---
# 贝尔曼方程

我们知道在强化学习中有几个概念需要关注，价值函数，动作价值函数，策略。

# 策略梯度定理

首先要知道的是，策略$\pi(\theta)$本身是没有好坏的评价的，我们赋予策略的好与坏实际上是在说这个策略导致的最终收益的大小。因此在说策略梯度以前我们需要弄清楚回报函数的形式$L(\theta)$。

一般地，回报函数定义成

$$

J(\theta) = \sum_{s\in \mathcal{S}}{d_\pi(s)V_\pi(s)} = \sum_{s\in \mathcal{S}}{d_\pi(d)\sum_{a\in \mathcal{A}}{\pi_\theta(a|s)Q_\pi(s,a)}}

$$

其中$d_\pi(s)$是马尔可夫链状态$s$在策略$\pi$的平稳分布，即$d_\pi(s) = \lim_{t\rightarrow \infty}{P(s_t=s\vert s_0,\pi)}$

要计算梯度$\nabla_\theta J(\theta)$有点困难，因为它不仅依赖于动作选择($\pi_\theta$)，同时还依赖平稳分布$d_\pi(s)$，这两个方面直接或者间接影响梯度的计算。

不过还好有一个**策略梯度定理**可以简化这一计算，先写结论：

$$
\begin{aligned}
    \nabla_\theta J(\theta) &= \nabla_\theta \sum_{s\in \mathcal{S}}{d_\pi(s)\sum_{a\in \mathcal{A}}{Q_\pi(s,a)\pi_\theta(a\vert s)}}\\
    &\propto \sum_{s\in \mathcal{S}}{d_\pi(s)\sum_{a\in \mathcal{A}}{Q_\pi(s,a)\nabla_\theta\pi_\theta(a\vert s)}}
\end{aligned}
$$

下面是证明过程，先从状态价值函数开始

$$
\begin{aligned}
    &\nabla_\theta V_\pi(s)\\
    =&\nabla_\theta\left(\sum_{a\in \mathcal{A}}{\pi_\theta(a\vert s)Q_\pi(s,a)}\right)\\
    =&\sum_{a\in \mathcal{A}}\Bigg({\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)+\pi_\theta(a\vert s)\nabla_\theta \color{red}{Q_\pi(s,a)}}\Bigg)&\text{微分乘法法则}\\
    = &\sum_{a\in \mathcal{A}}{\Bigg(\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a) + \pi_\theta(a\vert s)\nabla_\theta\color{red}{\sum_{s',r}{P(s',r\vert s,a)(r+V_\pi(s'))}}\Bigg)}&\text{使用下一状态价值展开}Q_\pi\\
    =&\sum_{a\in \mathcal{A}}{\Bigg(\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a) + \pi_\theta(a\vert s)\color{red}{\sum_{s',r}{P(s',r\vert s,a)\nabla_\theta V_\pi(s')}}\Bigg)}&r\text{和}P(s',r\vert s,a)\text{与}\pi_\theta\text{无关}\\
    =&\sum_{a\in \mathcal{A}}{\Bigg(\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a) + \pi_\theta(a\vert s)\color{red}{\sum_{s'}{P(s'\vert s,a)\nabla_\theta V_\pi(s')}}\Bigg)}&\text{因为}P(s'\vert s,a)=\sum_r{P(s',r\vert s,a)}&\\
\end{aligned}
$$

好，现在我们得到了一个等式

$$

\color{red}{\nabla_\theta V_\pi(s)} = \sum_{a\in \mathcal{A}}{\Bigg(\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a) + \pi_\theta(a\vert s)\sum_{s'}{P(s'\vert s,a)\color{red}{\nabla_\theta V_\pi(s')}}\Bigg)}
$$

注意，等式中红色部分是一个递推形式，这个等式可以一直展开下去。

我们现在想一下整个转移过程，记状态$s$以策略$\pi_\theta$经过$k$步转移到状态$x$的概率为$\rho_\pi(s\rightarrow x,k)$
- 当$k=0$时，$\rho_\pi(s\rightarrow s,k=0)=1$
- 当$k=1$时，$\rho_\pi(s\rightarrow s',k=1)=\sum_a{\pi_\theta(a\vert s)P(s'\vert s,a)}$，意味着从状态$s$转移到状态$s'$的过程为，状态$s$以概率$\pi_\theta(s)$选择了动作$a$，并且已知在状态$s$和动作$a$下，状态成为$s'$的概率是$P(s'\vert s,a)$
- 我们的目标是计算状态$s$经过$k+1$步转移到状态$x$上，那么我们可以递归这一过程为，状态$s$先经过$k$步转移到状态$s'$上，然后状态$s'$经过1步转移到状态$x$上，即$\rho_\pi(s\rightarrow x,k+1) = \sum_{s'}{\rho_\pi(s\rightarrow s',k)*\rho_\pi(s'\rightarrow x,k)}$

让我们回到$\nabla_\theta V_pi(s)$的表达式上。为了简便说明，我们记$\phi(s) = \sum_{a\in \mathcal{A}}{\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)}$。

我们将$\nabla_\theta V_\pi(s)$展开：

$$
\begin{aligned}
    &\nabla_\theta V_\pi(s)\\
    =&\phi(s) + \sum_{a\in \mathcal{A}}{\pi_\theta(a\vert s)\sum_{s'}{P(s'\vert s,a)\color{red}{\nabla_\theta V_\pi(s')}}}&\\
    =&\phi(s) + \sum_{s'}{\color{red}{\sum_{a}{\pi_\theta(a\vert s)P(s'\vert s,a)\nabla_\theta V_\pi(s')}}}\\
    =&\phi(s) + \sum_{s'}{\color{red}{\rho_\pi(s\rightarrow s',1)\nabla_\theta V_\pi(s')}}&\text{注意后面求和部分的后半部分，与前面的状态转移概率}k=1\text{比较}\\
    =&\phi(s) + \sum_{s'}{\rho_\pi(s\rightarrow s',1)\color{red}{\nabla_\theta V_\pi(s')}}\\
    =&\phi(s) + \sum_{s'}{\rho_\pi(s\rightarrow s',1)\color{red}{[\phi(s') + \sum_{s''}{\rho_\pi(s'\rightarrow s'',1)\color{red}{\nabla_\theta V_\pi(s'')}}]}}&\text{继续展开}V_\pi(s')\\
    =&\phi(s) + \sum_{s'}{\rho_\pi(s\rightarrow s',1)\phi(s')} + \color{red}{\sum_{s''}{\rho_\pi(s\rightarrow s'',2)\nabla_\theta V_\pi(s'')}}&\sum_{s'}{\rho_\pi(s\rightarrow s',1)\sum_{s''}{\rho_\pi(s'\rightarrow s'',1)}} = \sum_{s''}{\rho_\pi(s\rightarrow s',1)\sum_{s'}{\rho_\pi(s'\rightarrow s'',1)}}\\
    = &\phi(s) + \sum_{s'}{\rho_\pi(s\rightarrow s',1)\phi(s')} + \sum_{s''}{\rho_\pi(s\rightarrow s'',2)\phi(s'')}+\sum_{s''}{\rho_\pi(s\rightarrow s''',3)\color{red}{\nabla_\theta V_\pi(s''')}}\\
    = &\cdots\\
    = &\sum_{x\in\mathcal{S}}\sum_{k=0}^\infty{\rho_\pi(s\rightarrow x,k)\phi(x)}
\end{aligned}
$$

我们重新写目标函数$J(\theta)$

$$
\begin{aligned}
    \nabla_\theta J(\theta) &= \nabla_\theta V_pi(s_0)\\
    &=\sum_{s}{\sum_{k=0}^\infty{\rho_\pi(s_0\rightarrow s,k)\phi(s)}}\\
    &= \sum_{s}{\eta(s)\phi(s)}&\text{令}\eta(s) = \sum_{k=0}^\infty{\rho_\pi(s_0\rightarrow s,k)\phi(s)}\\
    &= \left(\sum_s{\eta(s)}\right)\sum_{s}{\frac{\eta(s)}{\sum_s{\eta(s)}}\phi(s)}&\\
    &\propto \sum_{s}{\frac{\eta(s)}{\sum_s{\eta(s)}}\phi(s)}&\\
    &=\sum_{s}{d_\pi(s)}\sum_{a}{\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)}&d_\pi(s)=\frac{\eta(s)}{\sum_s{\eta(s)}}\text{是平稳分布}\\
\end{aligned}
$$

梯度$\nabla_\theta J(\theta)$还可以写成

$$
\begin{aligned}
    \nabla_\theta J(\theta) &\propto \sum_{s}{d_\pi(s)}\sum_{a}{\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)}\\
    &=\sum_{s}{d_\pi(s)}\sum_{a}{\pi_\theta(a\vert s)Q_\pi(s,a)\frac{\nabla_\theta\pi_\theta(a\vert s)}{\pi_\theta(a\vert s)}}\\
    &=\mathbb{E}_\pi[Q_\pi(s,a)\nabla_\theta\ln{\pi_\theta(a\vert s)}]
\end{aligned}
$$

其中$$\mathbb{E}_\pi$$表示$$\mathbb{E}_{s\sim d_\pi,a\sim\pi_\theta}$$，即在策略$\pi$下状态和动作的分布的期望

对于所有的策略梯度的目标函数来说，都有这样的一个形式

$$
\nabla_\theta J(\theta) = \mathbb{E}_\pi[Q_\pi(s,a)\nabla_\theta\ln{\pi_\theta(a\vert s)}]
$$

这就是策略梯度定理